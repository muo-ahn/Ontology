# Vision–LLM–Ontology 파이프라인 요구사항

## 기능
- 기존/신규 의료 이미지를 모두 지원하며, 이미지 ID가 없으면 `Image` 노드를 생성하고 필요 시 환자·내원 정보를 연결한 뒤 추론 결과를 연계한다.
- 비전(VLM)·추론(LLM) 결과를 원본 이미지에 연결된 `AIInference` 노드로 영속화하고, 반복 호출 시 중복이 생기지 않도록 아이덴포턴시를 보장한다.
- 이미지 + 프롬프트를 입력받아 중간/최종 결과를 반환하는 API를 제공하고, 비동기 모드로 UI가 진행 상태를 스트리밍할 수 있도록 한다.
- 새로운 추론이 생성될 때마다 텍스트/이미지 임베딩을 Qdrant에 저장하고 일관되게 활용한다.

## 데이터 & 그래프
- `Patient`, `Encounter`, `Image`, `AIInference` 등에 Neo4j 제약 조건을 적용하고 모든 쓰기에 `MERGE`를 사용해 안전하게 업서트한다.
- 메타데이터가 불완전할 때(예: 환자 ID 누락) 관계 생성은 보류하면서도 나중에 연결할 수 있도록 상태를 기록한다.
- 모델명, 작업, 온도, 타임스탬프, 신뢰도, 아이덴포턴시 키 등 추론 메타데이터를 저장한다.

## 오케스트레이션
- VLM, LLM, 그래프 업데이트를 분리한 이벤트 기반 워크플로로 전환한다.
- Redis Streams 기반으로 `image.received`, `vision.captions`, `nlp.interpretation`, `graph.upserted` 토픽을 구성하고 컨슈머 그룹을 설정한다.
- SSE/WebSocket 엔드포인트를 제공해 UI가 요청 단위 진행 상황을 구독하도록 한다.

## 안정성 & 가시성
- `idempotency_key` 별 처리 상태(queued → captioned → interpreted → persisted)를 추적한다.
- 단계별 지연, 실패 횟수, 재시도 수 등 메트릭과 주요 식별자가 포함된 로그를 기록한다.
- 반복 실패 이벤트는 데드레터 큐로 라우팅하고 재시도/백오프 전략을 적용한다.

## 도구 & 개발 경험
- Docker Compose에 Redis 및 백그라운드 워커 컨테이너를 추가한다.
- Neo4j 시드, 워커 구동, 이벤트 재생을 위한 스크립트/Make 타깃을 제공한다.
- 파이프라인, 데이터 계약, 테스트 전략을 문서화해 기여자 온보딩을 지원한다.
